%\documentclass[10pt,a4paper,oneside]{article}
% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{article}

%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[total={7in,9in}]{geometry}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[bookmarks, colorlinks=false, pdftitle={Permutation and Lookup Arguments using Logarithmic Derivatives}, pdfauthor={author}]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{tikz}
\usepackage{titlesec}
\usepackage{float}
\usetikzlibrary{shapes, fit}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\setlength{\marginparwidth}{3cm}



\usepackage{url}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{nicefrac}

\usepackage[n,advantage,operators,sets,adversary,landau,probability,notions,logic,ff,mm,primitives,events, complexity,asymptotics,keys]{cryptocode}

\usepackage{listings}
\usepackage{footnote}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{%frame=tb,https://www.overleaf.com/project/608bc77c801b16bbadb2210a
  language=sh,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\RequirePackage{etex}

% Theorem environments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{thm}{Theorem}[]
\newtheorem*{thm*}{Theorem}
\newtheorem{cor}{Corollary}[]
\newtheorem{lem}[]{Lemma}
\newtheorem{prop}[]{Proposition}
\newtheorem{conj}[]{Conjecture}
\newtheorem{protocol}[]{Protocol}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{defn*}{Definition}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{rem*}[]{Remark}

% MATH %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\DeclareMathOperator{\N}{\mathbb{N}}
\renewcommand{\PP}{\mathbf{P}}
\newcommand{\OO}{\mathcal{O}}


\DeclareMathOperator{\param}{\mathsf{Par}}
\DeclareMathOperator{\gen}{\mathsf{Gen}}
\DeclareMathOperator{\setup}{\mathsf{Setup}}
\DeclareMathOperator{\indexer}{\mathsf{Index}}
\DeclareMathOperator{\comm}{\mathsf{Com}}
\DeclareMathOperator{\open}{\mathsf{Open}}
\DeclareMathOperator{\prove}{\mathsf{Prove}}
\DeclareMathOperator{\extract}{\mathsf{Extract}}
\DeclareMathOperator{\simulate}{\mathsf{Sim}}
\DeclareMathOperator{\RS}{\mathsf{RS}}
\DeclareMathOperator{\FFT}{\mathsf{FFT}}
\DeclareMathOperator{\Quotient}{\mathsf{Quotient}}
\DeclareMathOperator{\agree}{\mathsf{agree}}

\renewcommand{\adv}{\mathsf{Adv}}


\author{%
Ulrich Hab{\"o}ck
\\\\
Orbis Labs
\\
\texttt{team@orbislabs.com}
}

\begin{document}
%\frontmatter
\title{%
Multivariate lookups based on
logarithmic derivatives
}
\date{%
\today
}
\maketitle

%\setlength{\parskip}{5mm}


\begin{abstract}
Logarithmic derivatives translate products of linear factors into sums of their reciprocals, turning zeroes into simple poles of same multiplicity.
Based on this simple fact, we construct an interactive oracle proof for multi-column lookups over the boolean hypercube, which makes use of a single multiplicity function instead of working with a rearranged union of table and witnesses.
For single-column lookups the performance is comparable to the well-known \cite{Plookup} strategy used by Hyperplonk+ \cite{Hyperplonk}.
However, the real power of our argument unfolds in the case of batch lookups when multiple columns are subject to a single-table lookup:  
While the number of field operations is comparable to the Hyperplonk+ lookup (extended to multiple columns), the oracles provided by our prover are much less expensive. 
For example, for columns of length $2^{12}$, paper-pencil operation counts indicate that the logarithmic derivative lookup is between $1.5$ and $4$ times faster, depending on the number of columns.
\end{abstract}

%Keywords: SNARKs, recursive proofs, aggregation scheme

%\begin{KeepFromToc}
 \tableofcontents
%\end{KeepFromToc}

%\mainmatter
\section{Introduction}

Lookup arguments prove a sequence of values being member of an, often prediscribed, table. 
They are an essential tool for improving the efficiency of SNARKs for statements which are otherwise expensive to arithmetize. 
Main applications are lookups for relations of high algebraic complexity, and interval ranges, which are extensively used by zero-knowledge virtual machines enforcing execution trace elements being valid machine words.
Although closely related to permutation arguments \cite{shuffle, RAMs}, a first explicit occurence of lookups dates back to \cite{Arya}.
The break-through was achieved by Plookup \cite{Plookup}, a permutation argument based argument which improved over the one from \cite{Arya} and provided the first solution for arbitrary tables. 
Since then Plookup (and slight variants of it) is \textit{the} general purpose lookup argument used in many practical applications, for example \cite{Aztek, Halo2, Arkworks, Cairo, Miden}. 
%That strategy, which we call the Plookup strategy in the sequel, uses a rearranged concatenation of witness and table sequence

In this paper we describe a lookup argument based which is based on logarithmic derivatives instead of permutation arguments. 
As in classical calculus, formal logarithmic derivatives turn products $\prod_{i=1}^N (X - z_i)$ into sums of their reciprocals, 
\[
\sum_{i=1}^N \frac{1}{X - z_i},
\]
having poles with the same multiplicity as the zeros of the product.
Working with poles instead of zeros is extremly useful for lookup arguments.
%This fractional decomposition of the logarithmic derivative is extremly useful for lookup arguments:
While strategies for arguments about radicals are far from obvious, they turn trivial with logarithmic derivatives.
Concretely, given a sequence of field elements $(a_i)_{i=1}^N$ and another sequence $(t_j)_{j=1}^M$, then $\{a_i: i =1,\ldots, N\}\subseteq \{t_j : j=1,\ldots, M\}$ as sets, if and only if there exists a sequence of field elements $(m_j)_{j=1}^M$ (the multiplicities) such that
\begin{equation*}
\label{e:intro:lookup:eq}
\sum_{i=1}^N \frac{1}{X - a_i}  = \sum_{j=1}^M \frac{m_j}{X - t_j}.
\end{equation*}
 (This holds under quite mild conditions on the field, see Lemma \ref{lem:batchsetmembership} for details.)
%In particular for multi-column lookups, where several columns are subject to the same table, 
Based on this fractional identity we construct lookup protocols which are more  efficient than the Plookup approach, which argues via a sorted joining of witness and table sequence.
This is particularly true in the case of \textit{multi-column} lookups, where several sequences (``columns'') are subject to the same table lookup.
%Independent on the number of columns to be looked up, one still works with a single multiplicity function. 
In our lookup the oracle costs, measuring the number and sizes of the oracles the prover needs to provide, are significantly lower than for a lookup based on the Plookup strategy.
For large numbers of columns it is the half, while the arithmetic costs of the interactive oracle prover remain comparable.
%Although our implementation is not ready yet, we compare the two strategies by detailed operation counts, using a benchmark-backed measure for the cost of multi-scalar multiplications in terms of field multiplications.  
%For columns of length $2^{12}$, and considering an elliptic curve over a 256 bit large prime field, our counts indicate a speedup by a factor between $1.5$ and $2.5$, depending on $M$ the number of columns. 
%(For a large numbers of columns this factor is independent of $M$ and about $1.8$.) 
%Although we describe our lookups in the multivariate setting, we point out  

We stress the fact that we are not the only ones who exploit fractional decompositions for lookups. 
Concurrently, improving the work of \cite{Caulk} and \cite{CaulkPlus}, Gabizon et al. \cite{flookup} use fractional decompositions for ``large-table'' lookups, the proving-time of which is independent of the table size. 
%Although large-table lookups is a more advanced topic, 
This use case is perpendicular to ours.
We focus on multi-column lookups with respect to a single, in practice medium-sized table, a use case that is extensively used in execution trace proofs.

The document is organized as follows. 
In Section \ref{s:preliminaries}, we gather the preliminaries used in the sequel: 
The Lagrange kernel over the boolean hypercube,  basic facts on the formal logarithmic derivative, and a summary of the multivariate sumcheck argument. 
Besides that, we introduce Lagrange interactive oracle proofs, an oracle model we consider the best fit for arguments which are based on the Lagrange representation of polynomials rather than their coefficients.   
In Section \ref{s:lookups} we describe our multi-column lookup based on the logarithmic derivative. 
The protocol comes in two variants, one for a ``small'' number of columns, and another one which performs better for large numbers of columns (and is asymptotically linear in the instance size). 
For comparison reasons, we add an extra section (Section \ref{s:hyperplonk}) in which we sketch multi-column lookups using the Plookup strategy, adapted to the boolean hypercube. 
These rely on the time shift from Hyperplonk \cite{Hyperplonk}, and we consider them state-of-the-art in the multivariate setting.
%Eventually, in Appendix \ref{s:appendix} we recap the univariate Plookup argument, sketch inner product arguments for Lagrange queries,  and show how to turn the multivariate KZG \cite{Kate} commitment scheme into a scheme for Lagrange queries, which does not access the coefficients at any point in time.

We finally point out, that although our protocols are written for the multilinear setting,  their translation into univariate proofs is straight-forward.
We expect these univariate arguments to improve similarly over multi-column lookups based on the Plookup strategy.






\section{Preliminaries}
\label{s:preliminaries}

\subsection{The Lagrange kernel of the boolean hypercube}

Let $F$ denote a finite field, and $F^*$ its multiplicative group.
Throughout the document we regard the boolean hypercube $H= \{\pm 1\}^n$  as a multiplicative subgroup of  $(F^*)^n$.
For a multivariate functions $f(X_1,\ldots, X_n)$, we will often use the vector notation $\vec X = (X_1,\ldots, X_n)$ for its arguments, writing $f(\vec X) := f(X_1,\ldots, X_n)$.
 

%Given a function $f: H\rightarrow F$, its \textit{Langrange interpolation} is the unique multilinear polynomial $p(\vec X)$ in $\vec X = (X_1,\ldots, X_n)$ such that $p(\vec x) = f(\vec x)$ for every $\vec x\in H$.
%As $H$ has an increasing sequence of subgroups $\{1\} = H_0\subset H_1 \subset \ldots \subset H_n = H$, each $H_i$ having order $|H_i| = 2^i$,  Lagrange interpolation can be done in only $2^n$ field multiplications\footnotemark and $n\cdot 2^n$ additions and substractions, compared to $n\cdot 2^{n-1}$ multiplications and additions as for univariate interpolation from order $2^n$ subgroups of $F^*$.
%\footnotetext{%
%The field multiplications are due to the normalization of $f$ by the factor $\frac{1}{2^n}$, and can be entirely omitted an IOP. 
%One ``commits'' to the non-normalized Lagrange interpolant and corrects the queried evaluations.
%}%
%See Appendix \ref{s:appendix} for details. 
The \textit{Lagrange kernel} of $H$ is the multilinear polynomial
\begin{equation}
\label{e:LagrangeKernel}
L_H(\vec X, \vec Y)  = \frac{1}{2^n}\cdot \prod_{j=1}^n (1 + X_j\cdot Y_j).
\end{equation}
Notice that $L_H(\vec X, \vec Y)$ is symmetric in $\vec X$ and $\vec Y$, i.e. $L_H(\vec X, \vec Y)=L_H(\vec Y, \vec X)$, and that \eqref{e:LagrangeKernel} is evaluated within only $\bigO{\log|H|}$ field operations.
Whenever $\vec y \in H$ we have that $L_H(\vec X, \vec y)$ is the Lagrange polynomial on $H$, which is the unique multilinear polynomial which satisfies $L_H(\vec x, \vec y) = 1$ at $\vec x = \vec y$, and zero elsewhere on $H$.
In particular for a function $f: H\rightarrow F$  the inner product evaluation formula 
\[
\left\langle f ,L_H(\:.\:, \vec y)\right\rangle_H := \sum_{\vec x\in H} f(\vec x) \cdot L_H(\vec x, \vec y) = f(\vec y).
\]
is valid for every $\vec y\in H$.
This property extends beyond $H$, as the following Lemma shows.
\begin{lem}
\label{lem:Lagrange}
Let $p(\vec X)$ be the unique multilinear extension of $f: H\rightarrow F$. 
Then for every $\vec y\in F^n$,
\begin{equation}
\label{e:LagrangeScalarProduct}
\left\langle f ,L_H(\:.\:, \vec y)\right\rangle_H = \sum_{x\in H} f(\vec x) \cdot L_H(\vec x, \vec y) = p(\vec y).
\end{equation}
\end{lem}
\begin{proof}
%This is straight forward from the Lagrange representation $p(X)=\sum_{\vec z\in H} f(\vec z) \cdot L_H(\:.\:, \vec z)$.
Since $p(\vec y) = \sum_{\vec x\in H} f(\vec z)\cdot L_H(\vec X,\vec z)$, it suffices to show the claim for $p(X) = L_H(\vec X,\vec z)$, with $\vec z\in H$.
By the property of $L_H(\vec X,\vec z)$, we have $\big\langle L_H(\:.\:, \vec z), L_H(\:.\:,\vec y) \big\rangle_H =L_H(\vec y,\vec z)$, which by symmetry is equal to $L_H(\vec X,\vec y)$ at $\vec X=\vec z$.
This completes the proof of the Lemma.
\end{proof}


%This is tantamount to the tensor-query to point-query paradigm from \cite{TensorIOP} used in a line of work, e.g. \cite{TensorCodes, TensorR1CS, TensorRothblum, TensorR1CSarbitraryF}.
Note that for any $\vec y\in F^n$, the domain evaluation of $L_H(\vec X, \vec y)$ over $H$ can be computed in 
$\bigO{|H|}$ field operations, by recursively computing the domain evaluation of the partial products 
$
p_k(X_1,\ldots, X_k, y_1,\ldots, y_k)= \frac{1}{2^n}\cdot \prod_{j=1}^k (1 + X_j\cdot y_j)
$ 
over $H_k =\{\pm 1\}^k$ from the domain evaluation of $p_{k-1}$, where one starts with $f_0 = \frac{1}{2^n}$ over the single-point domain $H_0$.
Each recursion step costs $|H_{k-1}|$ field multiplications, denoted by $\mathsf M$, and the same number of additions, denoted by $\mathsf A$,  yielding overall
\begin{equation}
\label{e:lagrange:cost}
\sum_{k=1}^{n} |H_{k-1}| \cdot (\textsf M + \textsf A) < |H| \cdot  (\textsf M + \textsf A).
\end{equation}


\subsection{The formal derivate}

Given a univariate polynomial $p(X) =\sum_{k=0}^{d} c_k\cdot X^k$ over a general (possibly infinite) field $F$, its \textit{derivative} is defined as 
\begin{equation}
\label{e:DerivativePoly}
p'(X) := \sum_{k=1}^{d} k \cdot c_k \cdot X^{k-1}.
\end{equation}
As in calculus, the derivative is linear, i.e. for every two polynomials $p_1(X), p_1(X)\in F[X]$, and coefficients $\lambda_1,\lambda_2\in F$,
\begin{equation*}
%\label{e:DerivativeLinear}
(\lambda_1 \cdot p_1(X) + \lambda_2 \cdot p_1(X))' = \lambda_1\cdot p_1'(X) + \lambda_2\cdot p_2'(X)
\end{equation*}
 and we have the product rule
\begin{equation*}
%\label{e:ProductRule}
(p_1(X)\cdot p_2(X))' = p_1'(X)\cdot p_2(X) + p_1(X)\cdot p_2'(X).
\end{equation*}
For a function $\frac{p(X)}{q(X)}$ from the rational function field $F(X)$, the derivative is defined as the rational function
\begin{equation}
\label{e:DerivativeQuotient}
\left(\frac{p(X)}{q(X)}\right)' := \frac{p'(X)\cdot q(X) - p(X)\cdot q'(X)}{q(X)^2}.
\end{equation}
By the product rule for polynomials, the definition does not depend on the representation of $\frac{p(X)}{q(X)}$.
Both linearity as well as the product rule extend to rational functions. 
%For a proof of these facts, as well as alternative definitions for the formal derivative, we refer to standard literature. 

For any polynomial $p(X)\in F[X]$, if $p'(X)=0$ then $p(X)= g(X^p)$ where $p$ is the characteristic of the field $F$.
In particular, if $\deg p(X) < p$, then the polynomial must be constant.
As the analogous fact for fractions is not as commonly known, we give a proof of the following lemma.

\begin{lem}
\label{lem:DerivativeFraction}
Let  $F$ be a field of characteristic $p\neq 0$, and $\frac{p(X)}{q(X)}$ a rational function over $F$ with both  $\deg p(X) < p$ and $\deg q(X) < p$.
If the formal derivative $\left(\frac{p(X)}{q(X)}\right)' = 0$, then $\frac{p(X)}{q(X)} = c$ for some constant $c\in F$.
\end{lem}

\begin{proof}
If $q(X)$ is a constant, then the assertion of the Lemma follows from the corresponding statement for polynomials.
Hence we assume that $\deg q(X)>0$.
Use polynomial division to obtain the representation
\[
\frac{p(X)}{q(X)} = m(X) + \frac{r(X)}{q(X)},
\]
with $m(X), r(X) \in F[X]$, $\deg m(X) \leq \deg p(X)$, and $\deg r(X) < \deg q(X)$ whenever $r(X)\neq 0$.
By linearity of the derivative, we have
$
0 =  \left(\frac{p(X)}{q(X)}\right)' = m'(X) + \left(\frac{r(X)}{q(X)}\right)',
$
and therefore
%\[
%\frac{r'(X)\cdot q(X) - r(X)\cdot q'(X)}{q(X)^2} = - m'(X)
%\]
\begin{equation}
\label{e:der}
r'(X)\cdot q(X) - r(X)\cdot q'(X) = - m'(X)\cdot q(X)^2.
\end{equation}
Comparing the degrees of left and right hand side in \eqref{e:der}, we conclude that  $m'(X) = 0$.
Since $\deg m(X) \leq  \deg p(X) < p$ we have $m(X)= c$ for some constant\footnotemark $c\in F$. 
\footnotetext{%
For general degrees of $p(X)$ we would only be able to conclude that $m(X) = g(X^p)$ for some polynomial $g(X)$. 
}% 
Furthermore, if we had $r(X)\neq 0$ then the leading term of the left hand side in \eqref{e:der} would be
\[
%m\cdot d_m \cdot X^{m-1}\cdot c_n \cdot X^n + d_m \cdot X^{m-1}\cdot n\cdot  c_n \cdot X^{n-1} = 
(k - n) \cdot c_n\cdot d_{k} \cdot X^{n + k - 1},
\]
with $c_n \cdot X^n$, $n>0$, being the leading term of $q(X)$, and  $d_k \cdot X^k$, $0\leq k < n$, the leading term of $r(X)$.
As  $0 < n - k < p$, and both $c_n\neq 0$ and $c_m\neq 0$, the leading term of the left hand side of \eqref{e:der} would not vanish.
Therefore it must hold that  $r(X) = 0$ and the proof of the lemma is complete.
\end{proof}


\subsection{The  logarithmic derivative}

The \textit{logarithmic derivate} of a polynomial $p(X)$ over a (general) field $F$ is the rational function
\begin{equation*}
\frac{p'(X)}{p(X)}.
\end{equation*}
Note that the logarithmic derivative of the product $p_1(X)\cdot p_2(X)$ of two polynomials $p_1(X), p_2(X)$ equals the sum of their logarithmic derivatives, since by the product rule we have 
\[
\frac{(p_1(X)\cdot p_2(X))'}{p_1(X)\cdot p_2(X)} = \frac{p_1'(X)\cdot p_2(X) + p_1(X)\cdot p_2'(X)}{p_1(X)\cdot p_2(X)} 
= \frac{p_1'(X)}{p_1(X)} + \frac{p_2'(X)}{p_2(X)}.
\]
In particular the logarithmic derivative of a product $p(X) = \prod_{i=1}^n (X + z_i)$, with each $z_i\in F$, is equal to the sum
\begin{equation}
\label{e:LogDerivativeProduct}
\frac{p'(X)}{p(X)} %= \sum_{i=1}^n \frac{\prod_{j\in \{1,\ldots, n\}\setminus \{i\}} (X - z_j) }{p(X)} 
= \sum_{i=1}^n \frac{1}{X + z_i}.
\end{equation}

The following lemma is a simple consequence of Lemma \ref{lem:DerivativeFraction} and essentially states that, under quite mild conditions on the field $F$, if two normalized polynomials have the same logarithmic derivative then they are equal. 
We state this fact for our use case of product representations.
\begin{lem}
\label{lem:LogarithmicDerivative}
Let $(a_i)_{i=1}^n$ and $(b_i)_{i=1}^n$ be sequences  over a field $F$ with characteristic $p > n$. 
Then
$
\prod_{i=1}^n \left(X + a_i \right) =\prod_{i=1}^n \left(X + b_i \right)
$
in $F[X]$ if and only if  
\begin{equation*}
%\label{e:LogDerivativeSum}
\sum_{i=1}^n \frac{1}{X + a_i} =\sum_{i=1} ^n\frac{1}{X + b_i}
\end{equation*}
in the rational function field $F(X)$.
\end{lem}

\begin{proof}
If  $p_a(X) = \prod_{i=1}^n \left(X + a_i\right)$ and $p_b(X) = \prod_{i=1}^n \left(X + b_i\right)$
coincide, so do their logarithmic derivatives.
To show the other direction, assume that 
$
\frac{p_a'(X)}{p_a(X)}  = \frac{p_b'(X)}{p_b(X)}.
$
Then 
\[
\left(\frac{p_a(X)}{p_b(X)}\right)'  = \frac{p_a'(X)\cdot p_b(X) - p_a(X)\cdot  p_b'(X)} {p_b^2(X)} = 0.
\]
Hence by Lemma \ref{lem:DerivativeFraction} we have $\frac{p_a(X)}{p_b(X)} = c$ for some constant  $c \in F$.
As both $p_a(X)$ and $p_b(X)$ have leading coefficient equal to $1$, we conclude that $c =1$, and the proof of the Lemma is complete.
\end{proof}
\begin{rem}
\label{rem:LogarithmicDerivativeFunctionField}
We stress the fact that Lemma \ref{lem:LogarithmicDerivative} also applies to the case where $F$  is the function field $F_p(Y_1,\ldots, Y_k)$ over a finite field $F_p$ of characteristic $p$.
This observation will be useful when generalizing the permutation argument to the case where $a_i$ and $b_i$ are multilinear polynomials in $Y_1, \ldots, Y_n$.
\end{rem}

Given a product $p(X)=\prod_{i=1}^N (X + a_i)$ we can gather the poles of its logarithmic derivative obtaining the fractional decomposition  
\begin{align*}
\frac{p'(X)}{p(X)} = \sum_{a\in F} \frac{m(a)}{X + a},
\end{align*}
where $m(a)\in \{1,\ldots, N\}$ is the multiplicity of the value $a$ in $(a_i)_{i=1}^N$.
Fractional decompositions are unique, as shown by the following lemma.
\begin{lem}
\label{lem:UniqueFractionalRep}
Let $F$ be an arbitrary field and $m_1, m_2: F\rightarrow F$ any functions.
Then
$
\sum_{z\in F} \frac{m_1(z)}{X - z} = \sum_{z\in F} \frac{m_2(z)}{X - z}
$
in the rational function field $F(X)$, if and only if $m_1(z)=m_2(z)$ for every $z\in F$.
\end{lem}
\begin{proof}
Suppose that the fractional decompositions are equal.  
Then $\sum_{z\in F} \frac{m_1(z)-m_2(z)}{X - z} = 0$, and therefore
\[
p(X) = \prod_{w\in F} (X - w)\cdot\sum_{z\in F} \frac{m_1(z)-m_2(z)}{X - z} = \sum_{z\in F} (m_1(z)-m_2(z))\cdot \prod_{w\in F\setminus\{z\}} (X - w) = 0.
\]
In particular,
$
p(z) = (m_1(z) - m_2(z)) \cdot  \prod_{w\in F\setminus\{z\}} (z - w)= 0
$
for every $z\in F$.
Since   $\prod_{w\in F\setminus\{z\}} (z - w) \neq 0$ we must have $m_1(z)  = m_2(z)$ for every $z\in F$. 
The other direction is obvious.
\end{proof}

This leads to the following algebraic criterion for set membership, which is the key tool for our lookup arguments.
\begin{lem}[Set inclusion]
\label{lem:batchsetmembership}
Let $F$ be a field of characteristic $p>N$, and suppose that $(a_i)_{i=1}^N$, $(b_i)_{i=1}^N$ are arbitrary sequences of field elements.
Then $\{a_i \}\subseteq \{b_i\}$ as sets (with multiples of values removed), if and only if there exists a sequence $(m_i)_{i=1}^N$ of field elements from $F_q\subseteq F$ such that
\begin{equation}
\label{e:fracs}
\sum_{i=1}^N \frac{1}{X + a_i} = \sum_{i=1}^N \frac{m_i}{X + b_i}  
\end{equation}
in the function field $F(X)$.
Moreover, we have equality of the sets $\{a_i\} = \{b_i\}$, if and only if $m_i\neq 0$, for every $i=1,\ldots, N$.
\end{lem}

\begin{proof}%[Proof of Lemma \ref{lem:batchsetmembership}]
Let us denote by $m_a(z)$ the multiplicity of a field element $z$ in the sequence $(a_i)_{i=1}^N$.
Likewise, we do for $(b_i)_{i=1}^N$.
Note that since $N < p$, the multiplicities can be regarded as non-zero elements from $F_p$ as a subset of $F$.
Suppose that $\{a_i\}\subseteq \{b_i\}$ as sets. 
Set $(m_i)$ as the normalized multiplicities
$
m_i = \frac{m_a(b_i)}{m_b(b_i)}.
$
This choice of $(m_i)$ obviously satisfies \eqref{e:fracs}.

Conversely, suppose that \eqref{e:fracs} holds.
Collecting fractions with the same denominator we obtain fractional representations for both sides of the equation \eqref{e:fracs},  
\begin{align*}
\sum_{i=1}^N \frac{1}{X + a_i} &= \sum_{z\in F} \frac{m_a(z)}{X + z},
\\
\sum_{i=1}^N \frac{m_i}{X + b_i} & = \sum_{z\in F} \frac{\mu (z)}{X + z}.
\end{align*}
Note that since $N < p$, we know that for each $z\in \{a_i\}$ we have $m_a(z)\neq 0$. 
By the uniqueness of fractional representations,  Lemma \ref{lem:UniqueFractionalRep}, $m_a(z) = \mu(z)$ for every $z\in \{a_i\}$, and therefore each $z\in \{a_i\}$ must occur also in $\{b_i\}$. 
\end{proof}



\subsection{Lagrange interactive oracle proofs}

The oracle proofs of many general purpose SNARKs such as Plonk \cite{Plonk} or algebraic intermediate representations \cite{Starks} rely on witnesses that are given in Lagrange representation, i.e. by their values over a domain $H$.
Their multivariate variants may completely avoid the usage of fast Fourier transforms whenever the polynomial commitment scheme can be turned into one that does not need to know the coefficients, neither when computing a commitment nor in an opening proof.
Exactly this property is captured by \textit{Lagrange oracle proofs}, rather than polynomial ones \cite{Dark}.

A \textit{Lagrange interactive oracle proof} (\textit{Lagrange IOP}) over the boolean hypercube $H=\{\pm 1\}^n$ is an interactive protocol between two parties, the ``prover'' and the ``verifier''.
In each round, the verifier sends a message (typically a random challenge) and the prover computes one or several functions over the boolean hypercube, and gives the verifier oracle access to them.
From the moment on it is given access, the verifier is allowed to query the oracles for their inner products with the Lagrange kernel $L_H(\:.\:, \vec y)$, associated with an arbitrary vector $\vec y\in F^n$. 

The security notions for Lagrange IOPs, such as completeness, (knowledge) soundness, and zero-knowledge, are exactly the same as for other interactive oracle proofs.
We assume that the reader is familiear with these, and refer to \cite{IOPs} or \cite{Dark} for their formal definitions.

Lagrange IOPs are turned into arguments by instantiating the Lagrange oracles by a  \textit{Lagrange commitment scheme}.
A Lagrange commitment scheme is a commitment scheme for functions over $H$ that comes with an evaluation proof for Lagrange queries.
For example, inner product arguments \cite{BootleGroth} can be directly used to construct Lagrange commitment schemes,
% (see Appendix \ref{s:appendix: IPA}), 
but also the multilinear variant \cite{MVKZG} of the \cite{Kate} commitment scheme is easily modified to completely avoid dealing with coefficients. 
We suppose that this is well-known, and therefore we omit an explicit elaboration in this paper.


\subsection{The sumcheck protocol}

We give a concise summary on the sumcheck protocol \cite{sumcheck} for multivariate polynomials.
Given a multivariate polynomial $p(X_1,\ldots, X_n)\in F[X_1,\ldots, X_n]$, a prover wants to convince a verifier upon that
\begin{equation*}
s = \sum_{(x_1,\ldots, x_n) \in \{\pm 1\}^n} p(x_1, \ldots, x_n).
\end{equation*}
This is done by a random folding procedure which, starting with $H_0=\{\pm 1\}^n$, which stepwise reduces a claim on the sum over $H_i = \{\pm 1\}^{n-i}$, $i=0,\ldots, n-1$, to one over the hypercube $H_{i+1}$ of half the size. 
Eventually, one ends up with a claim over a single-point sum, which is paraphrased as the value of $p(X_1,\ldots, X_n)$ at a random point $(r_1,\ldots, r_n)\in F^n$ sampled in the course of the reduction steps.

%The reduction principle is best explained in the case of multilinear polynomials $p(X_1,\ldots, X_n)$.
%In the first round the prover provides the values of the partial sums
%\[
%s_1(x_1) = \sum_{(x_2,\ldots, x_n) \in \{\pm 1\}^{n-1}} p(x_1, x_2, \ldots, x_n),
%\]
%for $x_1\in\{\pm 1\}$, which the verifier checks to sum up to the claimed value, i.e. $v= s_1(-1) + s_1(+1)$. 
%If so,  the verifier samples a random $r_1\sample F$ and both prover and verifier continue on the protocol on the linear combination
%\begin{align*}
%r_1 \cdot s_1(+1) + (1 - r_1) \cdot s_1(-1) &= r_1\cdot\hspace*{-1cm}\sum_{(x_2,\ldots, x_n) \in \{\pm 1\}^{n-1}}  p( +1 , x_2, \ldots, x_n) +  (1-r_1)\cdot\hspace*{-1cm}\sum_{(x_2,\ldots, x_n) \in \{\pm 1\}^{n-1}}  p( -1 , x_2, \ldots, x_n)
%\\
%&= \sum_{(x_2,\ldots, x_n) \in \{\pm 1\}^{n-1}} p(r_1 , x_2, \ldots, x_n),
%\end{align*}
%where the latter equality holds as $p(X_1,\ldots, X_n)$ is a linear polynomial in $X_1$.
%After $n$ reduction steps of this kind, the initial claim is eventually reduced to the evaluation claim for $p(X_1,\ldots, X_n)$ at 
%$(X_1,\ldots, X_n) = (r_1, \ldots, r_n)$.
 
%\begin{protocol}[Sumcheck protocol, \cite{sumcheck}]
%Let $p(X_1,\ldots, X_n)$ be a multivariate polynomial over a finite field $F$. %with individual degrees $d_i=\deg_{X_i} p(X_1,\ldots, X_n)$.
%The sumcheck protocol, in which a prover wants to convince the verifier upon the sum $s = \sum_{(x_1,\ldots, x_n) \in \{\pm 1\}^n} p(x_1, \ldots, x_n)$, is as follows.
%\begin{itemize}
%\item 
%In the first round $i=1$, the prover sends (the coefficients of) the univariate polynomial 
%\[
%s_1(X) = \sum_{(x_{2},\ldots, x_n) \in \{\pm 1\}^{n-1}} p(X ,x_{2}, \ldots, x_n)
%\]
%of degree $d_1\leq \deg_{X_1} p(X_1,\ldots, X_n)$, to the verifier.
%%(This polynomial is computed in linear time from its values over a set $D_1\supseteq \{\pm 1\}$ of size $|D_1| = d_1 + 1$.) 
%The verifier checks if 
%\[
%v = s_1(-1) + s_1(+1),
%\] 
%and if so it responds with a random challenge $r_1$ sampled uniformly from $F$.
%
%\item
%In each of the further rounds $i=2,\ldots, n$, the prover sends the univariate polynomial of degree $d_i \leq \deg_{X_i} p(X_1,\ldots, X_n)$ given by
%\[
%s_i(X) = \sum_{(x_{i+1},\ldots, x_n) \in \{\pm 1\}^{n-i}} p(r_1,\ldots, r_{i-1},X ,x_{i+1}, \ldots, x_n),
%\]
%where $r_1, \ldots, r_{i-1}$ are the randomnesses received in the previous rounds.
%%(Again, the computation is done by interpolation from the values over a set $D_i\supseteq \{\pm 1\}$ of size $|D_i| = d_i + 1$.)
%The prover sends the coefficients of $s_{i}(X)$ to the verifier, which checks whether 
%\[
%s_{i-1}(r_{i-1}) = s_{i}(+1) + s_{i}(-1).
%\] 
%If so, the verifier sends another random challenge $r_i\sample F$ to the prover.
%\end{itemize}
%After these rounds the verifier checks if $s_n(r_n) = p(r_1,\ldots, r_n)$. 
%If so, the verifier accepts (otherwise it rejects).  
%\end{protocol}

\begin{protocol}[Sumcheck protocol, \cite{sumcheck}]
\label{p:Sumcheck}
Let $p(X_1,\ldots, X_n)$ be a multivariate polynomial over a finite field $F$. %with individual degrees $d_i=\deg_{X_i} p(X_1,\ldots, X_n)$.
The sumcheck protocol, in which a prover wants to convince the verifier upon the sum $s = \sum_{(x_1,\ldots, x_n) \in \{\pm 1\}^n} p(x_1, \ldots, x_n)$, is as follows.
We write $s_0(X)$ for the constant polynomial $s_0 =s$.
\begin{itemize}
\item
In each round $i=1,\ldots, n$, the prover sends the coefficients of the univariate polynomial 
\[
s_i(X) = \sum_{(x_{i+1},\ldots, x_n) \in \{\pm 1\}^{n-i}} p(r_1,\ldots, r_{i-1},X ,x_{i+1}, \ldots, x_n),
\]
of degree $d_i \leq \deg_{X_i} p(X_1,\ldots, X_n)$, where $r_1, \ldots, r_{i-1}$ are the randomnesses received in the previous rounds. (In the first round $i=1$ there are no previous randomnesses, and $p(r_1,\ldots, r_{i-1},X ,x_{i+1}, \ldots, x_n)$ is meant to denote $p(X,x_2,\ldots, x_n)$.)
%(Again, the computation is done by interpolation from the values over a set $D_i\supseteq \{\pm 1\}$ of size $|D_i| = d_i + 1$.)
The prover sends the coefficients of $s_{i}(X)$ to the verifier, which checks whether the received polynomial $s_i(X)$ is in fact of the expected degree and that
\[
s_{i-1}(r_{i-1}) = s_{i}(+1) + s_{i}(-1).
\] 
(Again, in the first round $i=1$ there is no $r_0$, and the verifier checks wheather $s_0 = s_1(+1) + s_1(-1)$.) 
If so, the verifier samples random challenge $r_i\sample F$ uniformly from $F$ and sends it to the prover.
\end{itemize}
After these rounds the verifier checks that $s_n(r_n) = p(r_1,\ldots, r_n)$. 
If so, the verifier accepts (otherwise it rejects).  
\end{protocol}


Soundness of the sumcheck protocol is proven by a repeated application of the Schwartz-Zippel lemma. 
We omit a proof, and refer to \cite{sumcheck} or \cite{SumcheckThaler}. 
\begin{thm}[\cite{sumcheck}]
The sumcheck protocol (Protocol \ref{p:Sumcheck}) has soundness error
\begin{equation}
\label{e:SumcheckSoundness}
\varepsilon_{sumcheck} \leq \frac{1}{|F|}\cdot \sum_{i=1}^n \deg_{X_i} p(X_1,\ldots, X_n).
\end{equation}
\end{thm}

The sumcheck protocol is easily extended to the sumcheck for a batch of polynomials $p_i(X_1,\ldots, X_n)$, $i=0, \ldots, L$, by letting the verifier sample a random vector $(\lambda_1,\ldots, \lambda_L)\sample F^L$, and a subsequent sumcheck protocol for the random linear combination
\[
\bar p (X_1, \ldots, X_n) = p_0(X_1,\ldots, X_n) + \sum_{i=1}^{L} \lambda_i \cdot p_i(X_1,\ldots, X_n).
\]
The soundness error bound increases only slightly,
\begin{equation}
\label{e:BatchSumcheckSoundness}
\varepsilon_{sumcheck} \leq \frac{1}{|F|}\cdot \left(1 + \sum_{i=1}^n \deg_{X_i} p(X_1,\ldots, X_n)\right).
\end{equation}

%\begin{rem}
\subsubsection*{Computational cost}

Let us discuss the prover cost of the sumcheck protocol for the case that  $p(\vec X) = p(X_1,\ldots, X_n)$ is of the form
\[
p(\vec X) = Q(w_1(\vec X), \ldots, w_m(\vec X)),
\]
with each $w_i(\vec X)\in F[X_1,\ldots, X_n]$ being multilinear, and 
\[
Q(Y_1,\ldots, Y_m) = \sum_{(i_1,\ldots, i_m)\in \{0,1\}^m} c_{i_1,\ldots, i_m} \cdot Y_1^{i_1}\cdots Y_m^{i_m}
\]
 is a multivariate polynomial having (a typically low) absolute degree $d$.
We denote the arithmetic complexity, i.e. the number of field multiplications $\mathsf M$, substractions $\mathsf S$ and additions $\mathsf A$ to evaluate $Q$ by $|Q|_\textsf M$, $|Q_\textsf S|$ and $|Q|_\textsf A$, respectively.
Each of the univariate polynomials $s_i(X)$, $i=1,\ldots, n$, is of degree at most $d$ the absolute degree of $Q$, and is computed from its values over a set $D\supseteq \{\pm 1\}$ of  size $|D| = d + 1$.
In each step $i=1,\ldots, n$, the values of $s_i(z)$ for $z\in D$ are obtained by linear interpolation of the domain evaluations of each
\[
w_j (r_1,\ldots, r_{i-1}, \pm 1, X_{i+1}, \ldots, X_n)
\]
over $H_{i}=\{\pm 1\}^{n-i}$ as given from the previous step, to the domain evaluation
\[
w_j (r_1,\ldots, r_{i-1}, z, X_{i+1}, \ldots, X_n), 
\]
the values of which are used for computing $s_i(z) = \sum_{(x_{i+1},\ldots, x_n)\in H_{i}} Q(r_1,\ldots, r_{i-1}, z, x_{i+1}, \ldots, x_n)$.
Given the random challenge $r_i$ from the verifier, the domain evaluation of each   
\[
w_j(r_1,\ldots, r_{i-1}, r_i, X_{i+1},\ldots, X_n)
\]
is computed by another linear interpolation.
Linear interpolation costs $|H_i|$ multiplications and the same number of additions/substractions for each multilinear polynomial, the values of $Q$ are obtained within $|Q|_\textsf M \cdot \textsf{M} +   |Q|_\textsf S \cdot \textsf S + |Q|_\textsf A \cdot \textsf A$.  
In terms of field multiplications $\mathsf M$, substractions $\mathsf S$ and additions $\mathsf A$, step $i$ consumes 
% Interpolation of m multilinear polynomials for |D_i| - 2 + 1 many points:
%     m*  |H_i| S for the differences
%     m * (|D_i| - 2) * |H_i| (M + A) for the domain evals for every z in D_i \setminus\{\pm 1\} 
%     m * |H_i| (M + A) for the domain eval at r_i
% Domain evaluation for Q at each point in D_i
%   |D_i| * |H_i| * (Q_M * M + Q_S * S + Q_A * A)
% Sum over |H_i| for Q at each point z in D_i
%  |D_i| * |H_i| * A
\begin{align*}
m\cdot |H_i|\cdot \textsf S + m \cdot (|D| - 1)\cdot |H_{i}| \cdot (\textsf M + \textsf A)
+  |D|\cdot |H_{i}| \cdot ( |Q|_\textsf M \cdot \textsf{M} +   |Q|_\textsf S \cdot \textsf S + |Q|_\textsf A \cdot \textsf A) 
+ |D|\cdot |H_{i}| \cdot \textsf A,
%\\
%< |D|  \cdot |H_{i}| \cdot \big((m + |Q|_M)\cdot\textsf M +  (m+ |Q|_\textsf S) \cdot\textsf S + (m + |Q|_\textsf A + 1)\cdot \textsf A\big).
\end{align*}
where the last term is for the domain sums.
Since $\sum_{i=1}^{n} |H_{i}| = |H| - 1$, the overall cost for the prover is bounded by 
\begin{equation}
\label{e:sumcheck:cost:precise}
|H|\cdot \left(1-\frac{1}{|H|}\right)\cdot \big( (d\cdot m + (d+1)\cdot |Q|_\textsf M)\cdot\textsf M +
 (m + (d + 1)\cdot |Q|_\textsf S) \cdot\textsf S +
(d\cdot m + (d+1)\cdot (|Q|_\textsf A + 1))\cdot\textsf A
\big).
\end{equation}
We shall use this formula for the operation counts of our lookup protocol.






\section{Lookups based on the logarithmic derivative}
\label{s:lookups}

Assume that $F$ is a finite field, and that $f_1, \ldots, f_M$ and  $t: H\rightarrow F$ are functions over the Boolean hypercube $H=\{\pm 1\}^n$. 
By Lemma \ref{lem:batchsetmembership}, it holds that $\bigcup_{i=1}^M \{f_i(\vec x)\}_{\vec x\in H}\subseteq \{t(\vec x)\}_{\vec x\in H}$ as sets, if and only if there exists a function $m: H\rightarrow F$ such that
\begin{equation}
\label{e:lookup:fractional:identity}
\sum_{\vec x\in H} \sum_{i=1}^M \frac{1}{X + f_i(\vec x)} = \sum_{\vec x\in H} \frac{m(\vec x)}{X + t(\vec x)},
\end{equation}
assuming that the characteristic of $F$ is larger than $M$ times the size of the hypercube.
If $t$ is injective (which is typically the case for lookup tables) then $m$ is the multiplicity function, counting the number of occurences for each value $t(\vec x)$ in $f_1,\ldots, f_M$ altogether, i.e.
$m(\vec x) = m_f(t(\vec x)) = \sum_{i=1}^M|\{\vec y \in H: f_i(\vec y) = t(\vec x))|$.
If $t$ is not one-to-one, we set $m$ as the \textit{normalized} multiplicity function 
\begin{equation}
\label{e:lookup:m}
m(\vec x) = 
\frac{m_f(t(\vec x))}{m_t(t(\vec x))} = \frac{ \sum_{i=1}^M |\{\vec y \in H: f_i(\vec y) = t(\vec x))|}{ |\{\vec y \in H: t(\vec y) = t(\vec x))|}.
\end{equation}
The plot for proving that $\bigcup_{i=1}^M \{f_i(\vec x)\}_{\vec x\in H}\subseteq \{t(\vec x)\}_{\vec x\in H}$ is as follows.
Given a random challenge $x\sample F$, the prover shows that the rational identity \eqref{e:lookup:fractional:identity} holds at $X= x$, whenever evaluation is possible. 
However, in order to make \eqref{e:lookup:fractional:identity} applicable to the sumcheck argument, the prover needs to provide multilinear helper functions for the rational expressions. 
We shall discuss two different different approaches to do that.
In  the first one, explained in Section \ref{s:lookup:small}, we use a single multilinear function for the entire fractional expression in \eqref{e:lookup:fractional:identity}, which is subject to a domain identity over $H$ which has $\bigO{M}$ variables and absolute degree $\bigO{M}$.
This will lead to a protocol with a $\bigO{M^2}$ prover. 
However, if $M$ is not too large this approach will be more performant than the second one, discussed in Section \ref{s:lookup:large}, in which we essentially use helper functions for each of the reciprocals $\frac{1}{x + f_i(\vec x)}$, and $\frac{1}{x + t(\vec x)}$. 
This second variant has a sumcheck polynomial in $\bigO{M}$ many variables, but the absolute degree is bounded, henceforth having a $\bigO M$ prover.


\subsection{An argument for not too many columns}
\label{s:lookup:small}

In this variant we provide a single helper function
\begin{equation}
\label{e:lookup:h}
h(\vec x) = \sum_{i=1}^M \frac{1}{x + f_i(\vec x)} - \frac{m(\vec x)}{x + t(\vec x)},
\end{equation}
subject $\sum_{\vec x\in H} h(\vec x) = 0$.
Correctness of $h$ is ensured by the domain identity
\begin{equation}
\label{e:lookup:h:identity}
\big(h(\vec x) \cdot  (x + t(\vec x)) + m(\vec x)\big) \cdot \prod_{i=1}^M (x + f_i(\vec x)) = (x + t(\vec x))\cdot \sum_{i=1}^M \prod_{j\neq i} (x + f_j(\vec x))
\end{equation}
over $H$, and we apply the Lagrange kernel $L_H(\:.\:, \vec z)$ at a randomly chosen $\vec z\sample F^n$ to reduce the domain identity to another sumcheck over $H$. 
Both sumchecks, the one for $h$ and the one for the domain identity, are then combined into a single one, using another randomness $\lambda\sample F$.

\begin{protocol}[Multi-column lookup over $H=\{\pm 1\}^n$]
\label{prot:lookup}
Let $M\geq 1$ be an integer, and $F$ a finite field with characteristic $p > M\cdot 2^n$. 
Given any functions $f_1, \ldots, f_M, t :H\rightarrow F$ on the boolean hypercube $H=\{\pm 1\}^n$, the Lagrange IOP for that $\bigcup_{i=1}^M\{f_i(\vec x) : \vec x\in H\}\subseteq \{t(\vec x) : \vec x\in H\}$ as sets is as follows.
\begin{enumerate} 
\item
The prover determines the (normalized) multiplicity function $m:H\rightarrow F$ as defined in \eqref{e:lookup:m},
and sends the oracle for $m$ to the verifier.
The verifier answers with a random sample $x\sample F\setminus \{- t(\vec x) : \vec x\in H\}$. 

\item
\label{i:lookup:step1}
Given the challenge $x$ from the verifier, the prover computes the randomized functions $\varphi_i(\vec x) = x + f_i(\vec x)$, $i=1,\ldots, M$, and $\tau(\vec x) =  x + t(\vec x)$.
It determines the values for
\begin{equation}
\label{e:lookup:h:phi}
h(\vec x) = \sum_{i=1}^M \frac{1}{\varphi_i(\vec x)} - \frac{m(\vec x)}{\tau(\vec x)},
\end{equation}
over $H$, and sends the oracle for $h$ to the verifier.

\item
\label{i:lookup:step2}
The verifier responds with a random vector $\vec z \sample F^n$ and a batching randomness $\lambda\sample F$.
Now, both prover and verifier engage in the sumcheck protocol (Protocol \ref{p:Sumcheck}) for 
\begin{align*} 
%\label{e:sumcheckh}
\sum_{\vec x \in H} Q(L_H(\vec x, \vec z),  h(\vec x), m(\vec x),  \varphi_1(\vec x), \ldots, \varphi_M(\vec x),  \tau(\vec x))&= 0,
\end{align*}
where %$Q$ is the degree $M+2$ polynomial
\begin{equation}
\label{e:lookup:Q}
Q(L,h,m, \varphi_1,\ldots, \varphi_M,  \tau) =   
L \cdot \left((h \cdot \tau + m) \cdot \prod_{i=1}^M \varphi_i - \tau\cdot \sum_{i=1}^M \prod_{j\neq i} \varphi_j\right)
+  \lambda \cdot h.
\end{equation}
The sumcheck protocol outputs the expected value $v$ for the multivariate polynomial 
\begin{equation}
\label{e:lookup:QinX}
\begin{aligned}
Q(L_H(\vec X, \vec z), h(\vec X), m(\vec X), \varphi_1(\vec X),\ldots, \varphi_M(\vec X),  \tau(\vec X))
\end{aligned}
\end{equation}
at $\vec X=\vec r$ sampled by the verifier in the course of the protocol.

\item
The verifier queries $[f_1], \ldots, [f_M], [t], [m], [h]$ for their inner product with $L_H(\:.\:,\vec r)$, and uses the answers 
to check whether \eqref{e:lookup:QinX} equals the expected value $v$ at $\vec X = \vec r$. 
(The value $L_H(\vec r, \vec z)$ is computed by the verifier.)
\end{enumerate}
\end{protocol}

\begin{rem}
\label{rem:lookup:completeness}
We imposed the condition $x\notin \{- t(\vec x)\}_{\vec x \in H}$ merely for completeness. 
However in some applications it may be not be desirable, or even not possible, to sample $x$ from outside the range of $t$.
There are several ways to handle this.
One can simply omit the constraint on $x$, letting the verifier sample $x\sample F$ and the prover set $h$ arbitrary whenever \eqref{e:lookup:h:phi} is not defined.
This comes at no extra cost, but the obtained protocol is only overwhelmingly complete. 
That is, with a probability of at most $\frac{|H|}{|F|}$ in the verifier randomness $x$, the honest prover does not succeed.
In practice this is often considered acceptable, and many lookup implementations have a non-zero completeness error. 
Whenever this is not acceptable, one may modify the domain identity \eqref{e:lookup:h:identity} to 
\begin{equation}
\label{e:lookup:h:identity:complete}
\left((h \cdot \tau + m) \cdot \prod_{i=1}^M \varphi_i - \tau\cdot \sum_{i=1}^M \prod_{j\neq i} \varphi_j\right)\cdot \tau\cdot  \prod_{i=1}^M \varphi_i  = 0
\end{equation}
over $H$, which imposes no condition on $h(\vec x)$ whenever $\tau(\vec x)= 0$. 
However, this approach comes at the cost of almost doubling the absolute degree of $Q$.
\end{rem}

Let us point out two variations of Protocol \ref{prot:lookup}.
In the single-column case $M=1$ the lookup argument can be turned into a multiset check for the ranges of $f_1$ and $t$, by setting $m$ as the constant function $m(\vec x) = 1$.
In this case only $h$ needs to be provided by the prover.
More interestingly, Protocol \ref{prot:lookup} is easily extended to a proof of range equality, showing that $\bigcup_{i=1}^M \{f_i(\vec x)\}_{\vec x\in H} = \{\tau (\vec x)\}_{\vec x\in H}$ as sets.
For this the prover additionally shows that $m \neq 0$ over $H$, which is done by providing another auxiliary function $h_m: H\rightarrow F$ subject to $h_m\cdot m = 1$ over $H$.
However, we are not aware of any application of this fact.






\subsection{A variant for a large number of columns}
\label{s:lookup:large}

%The polynomial $Q$ from \eqref{e:lookup:Q} has $\bigO{M}$ variables, and absolute degree $\bigO{M}$.
%This leads to a cost of the overall sumcheck which is \textit{quadratic} in $M$, and it will depend on the size of $M$ whether Protocol \ref{prot:lookup} performs better than a strategy which is linear in $M$, but which uses a linear number of oracles.
%Instead of doing an $M$-fold application of Protocol \ref{prot:lookup} for a single column, we will compare with following optimized strategy.
Assume that $M=2^m$, so that we can index the columns to be looked up by $f_{\vec z}$, where $\vec z\in \{\pm 1\}^m$.
We patch these columns into a single function $f$ over the extended hypercube $\bar H = \{\pm 1\}^m \times H$ by
\[
f(\vec y, \vec x) = \sum_{\vec z\in \{\pm 1\}^m} L_{m}(\vec y, \vec z)\cdot f_{\vec z}(\vec x),
\]
where $L_m(\vec y, \vec z)$ is the Lagrange polynomial for $\{\pm 1\}^m$. 
Given the random challange $x\sample F\setminus \{-t(\vec x) : \vec x\in H\}$ from the verifier, the prover supplies an oracle for the values of
\begin{equation}
\label{e:lookup:large:h}
h(\vec y, \vec x) = \frac{1}{x + f(\vec y, \vec x)} - \frac{\tilde m(\vec x)}{x + t(\vec x)}
\end{equation}
over the extended hypercube, where $\tilde m(\vec x)= \frac{1}{2^m}\cdot m(\vec x)$. 
The supplementary function $h$ is subject to the domain identity
%\[
%h(\vec y, \vec x) \cdot (x + f(\vec y, \vec x))\cdot (x + t(\vec x)) = x + t(\vec x)  - \tilde m(\vec x)\cdot (x + f(\vec y, \vec x))
%\]
\[
\big(h(\vec y, \vec x) \cdot (x + t(\vec x)) + \tilde m(\vec x) \big) \cdot (x + f(\vec y, \vec x)) - (x + t(\vec x)) = 0
\]
over $\bar H$, and 
\[
\sum_{\vec y \in \{\pm 1\}^m}\sum_{\vec x\in H} h(\vec y, \vec x) = 0.
\]
Again, the domain identity is turned into a sumcheck over $\bar H$ by applying the Lagrange kernel $L_{\bar H}(\:.\:, \vec z)$, where $\vec z$ is now sampled from $F^{m + n}$.
Combining the two sumchecks using a random $\lambda\sample F$ leads to the overall sumcheck  
\begin{align*} 
%\label{e:sumcheckh}
\sum_{\vec y \in\{\pm 1\}^m}\sum_{\vec x \in H} Q(L_{\bar H}((\vec y,\vec x), \vec z),  h(\vec y, \vec x), \tilde m(\vec x),  \varphi(\vec y, \vec x), \tau(\vec x))&= 0,
\end{align*}
over $\bar H=\{\pm 1\}^m \times H$, with $\varphi(\vec y, \vec x) = x + f(\vec y, \vec x)$ and $\tau(\vec x)= x + t(\vec x)$, and where 
$Q$ is 
\begin{equation}
\label{e:lookup:Q:linear}
Q(L,h, \tilde m, \varphi, \tau) =   
L \cdot \left((h \cdot \tau +\tilde  m) \cdot \varphi - \tau \right)
+  \lambda \cdot h.
\end{equation}
In this variant, providing $h$ amounts to $M$ oracles over $H$, yielding an overall equivalent of $M+1$ oracles of size $|H|$. 
However, $Q$ has only $\nu=5$ variables and its absolute degree is  independent of the number of columns.
%\end{rem}





\subsection{Soundness}
\label{s:lookup:soundness}

The soundness analysis of Protocol \ref{prot:lookup} is a straight-forward application of the Schwartz-Zippel lemma and the Lagrange-query to point-query correspondence stated by Lemma \ref{lem:Lagrange}.
We merely sketch it.
The univariate rational lookup identity \eqref{e:lookup:fractional:identity} is turned into a polynomial identity of degree at most $|H|\cdot (M+1) - 1$ by multiplying it with the common denominator  
\begin{equation}
\label{e:lookup:common:denominator}
p(X) = \prod_{\vec x\in H} (X + t(\vec x)) \cdot \prod_{i=1}^M (X + f_i(\vec x)).
\end{equation}
Since we sample $x$ from a set of size at least $|F|-|H|$, the soundness error of Step \ref{i:lookup:step1} of the protocol is at most 
\begin{equation}
\label{e:lookup:epsilon1}
\varepsilon_1 \leq \frac{(M+1)\cdot |H| - 1 }{|F|-|H|}.
\end{equation}
The soundness error due to the reduction of the domain identity  \eqref{e:lookup:h:identity}  to the Lagrange kernel based sumcheck is 
\[
\varepsilon_2 \leq \frac{1}{|F|},
\]
as scalar products with the Lagrange kernel translate to point evaluation of the multilinear extension.
This yields the following theorem. 

\begin{thm}
\label{thm:lookup:soundness}
 The interactive oracle proof described Protocol \ref{prot:lookup} has soundness error
\[
\varepsilon < \frac{(M+1)\cdot |H| - 1 }{|F|-|H|} + \varepsilon_{sumcheck},
\]
where $\varepsilon_{sumcheck}$ is the soundness error of the sumcheck argument \eqref{e:BatchSumcheckSoundness} over $H$ for a multivariate polynomial in $M+4$ variables with maximum individual degree $M+3$.
\end{thm}
\begin{rem}
The $\bigO M$-variant described in Section \ref{prot:lookup} has same soundness error, with $\varepsilon_{sumcheck}$ being the soundness error of the sumcheck argument over the extended hypercube of size $M\cdot |H|$ for a multivariate polynomial in $\nu=5$ variables and maximum individual degree $4$. 
\end{rem}

\begin{rem}
\label{s:pa:Generalizations}
Protocol \ref{prot:lookup} and its variant from Section \ref{s:lookup:large} are easily generalized to functions with multilinear values, 
\begin{align*}
t(\vec x) &= \sum_{(j_1,\ldots, j_k)\in\{0,1\}^k} t_{j_1,\ldots, j_k}(\vec x)\cdot Y_1^{i_1}\cdots Y_k^{j_k},
\\
f_i(\vec x) &= \sum_{(j_1,\ldots, j_k)\in\{0,1\}^k} f_{i, j_1,\ldots, j_k}(\vec x)\cdot Y_1^{i_1}\cdots Y_k^{j_k},
\end{align*}
$i=1,\ldots, M$, without changing the soundness error bound from Theorem \ref{thm:lookup:soundness}.
As $F[X, Y_1,\ldots, Y_k]$ is a unique factorization domain, and polynomials of the form $X -  \sum_{(i_1,\ldots, i_k)\in\{0,1\}^k} c_{i_1,\ldots, i_k}\cdot Y_1^{i_1}\cdots Y_k^{i_k}$ are irreducible, we may apply Lemma \ref{lem:batchsetmembership} to see that $\bigcup_{i=1}^M \{f_i(\vec x)\}_{\vec x\in H}\subseteq \{t(\vec x)\}_{\vec x\in H}$ as sets in the rational function field $F(X, Y_1,\ldots, Y_k)$, if and only if there exists a function $m: H\rightarrow F$ such that
\begin{equation}
\label{e:lookup:fractional:identity:general}
\sum_{\vec x\in H} \sum_{i=1}^M \frac{1}{X + f_i(\vec x)(\vec Y)} = \sum_{\vec x\in H} \frac{m(\vec x)}{X + t(\vec x)(\vec Y)}.
\end{equation}
The only change to Protocol \ref{prot:lookup} is that the verifier now samples $x$ from $F$ and $\vec y = (y_1,\ldots, y_k)$ from $F^k$, and continues with $x - f_i(\vec x)$ and $x - t(\vec x)$ replaced by $x - f_i(\vec x)(\vec y)$ and $x - t(\vec x)(\vec y)$.
%As the individual degrees with respect to $Y_1$, \ldots, $Y_k$ are again bounded by $|H|$, the soundness error does not change.
%Alternatively, at the cost of a slight increase of the soundness error one can choose $f, g: H\longrightarrow F[X]$ with values being polynomials of degree at most $k-1$ . 
\end{rem}

\subsection{Computational cost}
\label{s:lookup:cost}

The polynomial $Q$ from \eqref{e:lookup:Q} has $\nu= M + 4$ variables, and absolute degree $d = M + 3$.
Let us discuss an domain evaluation strategy for the values of $Q$, which makes use of batch inversion. 
This strategy allows us to evaluate $Q$ much more efficiently than using \eqref{e:lookup:Q}, but demands a modification of the sumcheck operation count formula \eqref{e:sumcheck:cost:precise}.
%Let us elaborate an evaluation strategy for $Q$, which makes use of 
% Arithmetic complexities, using the inverses of phi_1, ..., phi_{M-1}
% 	M  - 1 multiplications for p = phi_1 * ... * phi_{M-1} * phi_M
% 	M - 1 multiplications to obtain the partial products p_i = \prod_{j\neq i} \phi_j for i=1,..,M-1. (the product p_M is already known)
%	2 muls for m * p + tau * (h * p + \sum_{i} p_i)
%	another 2 muls for the product with L and lambda.
Assume that the inverses of $\varphi_1, \ldots, \varphi_{M-1}$ are given.
Then we may evaluate $Q$ by the fractional representation
\begin{align*}
%Q &= L\cdot  \prod_{i=1}^M\varphi_i\cdot \left(m + \tau\cdot \left(h  - \sum_{i=1}^M \frac{1}{\varphi_i} \right) \right) + \lambda\cdot h,
%\\
Q &= L\cdot  \prod_{i=1}^{M-1}\varphi_i\cdot \left(\varphi_M\cdot m + \tau\cdot \left(\varphi_M\cdot h - \left(\sum_{i=1}^{M-1} \frac{1}{\varphi_i} +1 \right)\right) \right) + \lambda\cdot h.
\end{align*}
This costs  $M + 4$ multiplications, one substraction, and $M+1$ additions, and hence
the arithmetic complexities are $|Q_\mathsf M| =  M +  4$, $|Q_\mathsf S| = 1$, $|Q_\mathsf A| = M + 1$.
Now, to attribute the inverses in formula \eqref{e:sumcheck:cost:precise}, we increase the multiplicative complexity by $3\cdot (M - 1)$, which represents the fractional cost of the batch inversion\footnotemark of $\varphi_1, \ldots, \varphi_{M-1}$. 
\footnotetext{%
Batch, or Montgomery inversion, of a sequence $(a_i)_{i=1}^N$ computes the inverses of $a_i^{-1}$ by recursively computing the cumulative products $p_i = a_1\cdot\ldots \cdot a_n$, $i=0,\ldots,n$, then calculating their inverses $q_i = \frac{1}{p_i}$ in a reverse manner   
starting with $q_n = \frac{1}{p_n}$, and putting $q_{i-1} = q_i \cdot a_i$, where $i$ goes from $n$ down to $1$. 
The inverses are then derived via $a_i^{-1} = p_{i-1}\cdot q_{i}$, where $p_0:=1$.
The overall cost of the batch inversion is $3\cdot (N-1)$ multiplications and a single inversion.
}%
This yields the following equivalent complexities
 \[
|Q_\mathsf M| = 4\cdot M + 1 , |Q_\mathsf S| = 1, |Q_\mathsf A| = M + 1,
\]
which we may plug into formula \eqref{e:sumcheck:cost:precise}.

Therefore the prover cost of Protocol \ref{prot:lookup} is as follows:
Given the values of $f_1, \ldots, f_M$ and $t$ over $H$, computing $\varphi_1 = x + f_1, \ldots, \phi_M = x + f_M$,  $\tau = x + t$ costs $|H|\cdot (M+ 1) \cdot\mathsf A$, and their reciprocals $\frac{1}{\varphi_1},  \ldots, \frac{1}{\varphi_M}$, $\frac{1}{\tau}$ are obtained within $3\cdot |H| \cdot (M+1) \cdot\mathsf M$, using batch inversion. 
With these reciprocals we obtain the values for
\[
h = \sum_{i=1}^M \frac{1}{\varphi_i} - \frac{m}{\tau} 
\]
by $|H|\cdot (1\cdot \mathsf S +(M-1)\cdot\mathsf A)$. 
By the remark following Lemma \ref{lem:Lagrange}, the values for $L_H(\vec X, \vec y)$ over $H$ are obtained within $|H|\cdot (\mathsf M + \mathsf A)$ operations. 
Hence the total cost of the preparation phase is
\[
|H|\cdot ((3\cdot M + 4)\cdot\mathsf M + 1\cdot\mathsf S + (2\cdot M + 1) \cdot\mathsf A.
\]
According to \eqref{e:sumcheck:cost:precise} the sumcheck costs  
\begin{equation*}
|H| \cdot \left(1-\frac{1}{2^{n}}\right)\cdot \big((5\cdot M^2 + 24 \cdot M + 16) \cdot\textsf M + (2\cdot M + 8)\cdot\textsf S + (2\cdot M^2 + 13\cdot M + 20) \cdot \textsf A\big).
\end{equation*}
However, as we may reuse the reciprocals of $\varphi_1,  \ldots, \varphi_{M-1}$ in the first step of the sumcheck, we correct the sumcheck cost by substracting $|H|\cdot (3\cdot (M-1))$. 
Neglecting the $\nicefrac{1}{2^{n}}$-term, the overall cost of the prover is 
\begin{equation}
\label{e:lookup:cost}
|H| \cdot  \big((5\cdot M^2 + 24 \cdot M + 23) \cdot\textsf M 
+ (2\cdot M + 9)\cdot\textsf S 
+ (2\cdot M^2 + 15\cdot M + 21) \cdot \textsf A\big),
\end{equation}
whereas it provides two $H$-sized oracles. 
The cost is $\bigO{|H|}$ but depends quadratically in $M$ the number of columns to be looked up. 
This quadratic occurence is due to the fact that both, the number of function as well as the degree of $Q$ grow linearly in $M$. 

The cost for the $\bigO{M}$ strategy from Section \ref{s:lookup:large} is as follows.
There, the prover provides $h$ over the extended hypercube of size $|\bar H|=M\times |H|$, and $Q$ from \eqref{e:lookup:Q:linear} has $\nu = 5$ variables, absolute degree $d=4$, and arithmetic complexities  $|Q_\mathsf M|= 4$, $|Q_\mathsf S|= 1$, $|Q_\mathsf A|=2$.
Computing the values of $h$ over $\bar H$ using batch inversion costs 
\[
M\cdot |H| \cdot(3 \cdot \mathsf M + \mathsf A),
\]
and the values for $L_{\bar H}(\:,\:, \vec z)$ over $\bar H$ are determined in $M\cdot |H|\cdot (\mathsf M + \mathsf A)$.
The overall sumcheck costs
\[
M\cdot |H| \cdot \left(1-\frac{1}{M\cdot |H|}\right)\cdot \big(40 \cdot \textsf M + 10\cdot\textsf S + 35 \cdot \textsf A\big).
\]
Neglecting the $\frac{1}{M\cdot |H|}$-term, the overall cost for the prover is
\begin{equation}
\label{e:lookup:large:cost}
M\cdot |H|\cdot (44\cdot\mathsf M + 10\cdot\mathsf S + 37\cdot\mathsf A),
\end{equation}
but the prover needs to provide the oracles for one function over  a domain of size $M\cdot|H|$, and one over $H$. 


Let us estimate the range for $M$ where Protocol \ref{prot:lookup} is more efficient than the $\bigO M$ variant. 
For this we use the benchmarks from Table \ref{tab:pippenger} which measure the equivalent number of field multiplication for a multi-scalar multiplication in an elliptic curve over $256$ bit large field.
Based on this equivalent, and our operation counts for the oracle prover, we obtain the following break even points.
\begin{table}[h!]
\caption{%
The estimated number of columns $M$ where the $\bigO{M}$ strategy starts to perform better than Protocol \ref{prot:lookup}. 
The numbers are based on the operation counts \eqref{e:lookup:cost} and \eqref{e:lookup:large:cost} for the oracle prover, and the 
benchmarks for a multi-scalar multiplication over the Pallas curve, see Table \ref{tab:pippenger}.
}
\vspace*{0.5cm}
\centering
\begin{tabular} {|c|c|c|c|c|}
\hline
$\log|H|$ & 12 & 14 & 16 & 18
\\\hline
$M$ & 114 & 95 & 81 & 73
\\\hline
\end{tabular}
\end{table}



%%
%% Poseidon hash comparison
%%
%As a lower bound for elliptic curve based schemes, we take the number of field operations for $x^5$-Poseidon with rate $r=2$, capacity $c=1$, $R_F=8$ full rounds and $R_p=57$ partial rounds. (These are the parameters from \cite{Poseidon} for a security level of $128$ bits over a $255$ bit large base field.) 
%Each permutation, which processes $r=2$ many elements costs
%$
%604\cdot\mathsf M + 591\cdot\mathsf A, 
%$ 
%using the optimized evaluation strategy from Appendix B in \cite{Poseidon}.
%Hence computing the hash of two functions over $H$ costs
%\[
%|H|\cdot (604\cdot\mathsf M + 591\cdot\mathsf A). 
%\]
%Therefore a $M$ single-column lookups cost 
%\begin{equation}
%\label{lookup:cost:Mcolumn:naive}
%|H|\cdot ((604\cdot M + 42)\cdot\mathsf M + 10\cdot\mathsf S + (591\cdot M + 30)\cdot\mathsf A),
%\end{equation}
%and the multi-column lookup for $M$ columns
%\begin{equation}
%\label{lookup:cost:Mcolumn:batch}
%|H| \cdot  \big((5\cdot M^2 + 19 \cdot M + 1226) \cdot\textsf M 
%+ (2\cdot M + 8)\cdot\textsf S 
%+ (2\cdot M^2 + 13\cdot M + 621) \cdot \textsf A\big).
%\end{equation}
%Comparing \eqref{lookup:cost:Mcolumn:naive} and \eqref{lookup:cost:Mcolumn:batch} yields  a maximum speedup by a factor of $5$ for $M=11$, more than double as fast for $3\leq M\leq 58$, and a break-even point at about $M=124$ columns.
%


%Compared to the cost of the univariate lookup PIOP (see Appendix \ref{s:uv:lookup}), the multivariate lookup has its break-even point at about $\log |H| \approx 2.3$.
%However, this is without taking into account that in the univariate case a total of $3$ polynomials of size $|H|$ need to be provided, which is one more than in our protocol (neglecting the overall quotient polynomial). 
%Thus it is safe to assume that Protocol \ref{prot:pa} is throughout faster than the univariate case, and we expected it to be more than three times as performant at $\log|H| \approx 12$.

\section{Lookups based on the Hyperplonk shift}
\label{s:hyperplonk}

In this section we informally discuss multi-column lookups using the time shift from Hyperplonk+ \cite{Hyperplonk}. 
We  give detailed operation counts and compare with our lookup arguments from Section \ref{s:lookups}.

Hyperplonk \cite{Hyperplonk} introduces a time shift $T: H\rightarrow H$ which is derived from the multiplication by a primitive root in $GF(2^n)$, 
\[
T(x_1, \ldots, x_n) = \frac{1 + x_n}{2} \cdot (1, x_1,\ldots, x_{n-1}) + \frac{1 - x_n }{2} \cdot (- 1,(-1)^{1 - c_1}\cdot x_1,\ldots, (-1)^{1 - c_{n - 1}}\cdot x_{n-1}),
\]  
where the $c_i\in\{0,1\}$ are the coefficients of a primitive polynomial $1 + \sum_{i=1}^{n-1} c_i\cdot X^i + X^n$ over $GF(2)$.
The time shift acts transitively on the punctuated hypercube $H' = H\setminus\{\vec 1\}$ (as a group automorphism it has $\vec 1$ as a fixed point), and more importantly, evaluations of a shifted function $f(T(\vec x))$ can be simulated from two evaluations of $f$ by
\begin{equation}
\label{e:hyperplonk:shifted:function}
f(T(x_1, \ldots, x_n)) =  \frac{1 + x_n}{2} \cdot f(1, x_1,\ldots, x_{n-1}) + \frac{1 - x_n }{2} \cdot f(- 1,(-1)^{1 - c_1}\cdot x_1,\ldots, (-1)^{1 - c_{n - 1}}\cdot x_{n-1}).
\end{equation}

%Although the Hyperplonk lookup uses the , 
Using the time shift $T$ allows for the same strategy as in the well-known univariate case \cite{Plookup}, which for the sake of completness recap in Appendix \ref{s:uv:lookup}.
In short, the argument is based on the fact that, given two sequences of field elements $(a_i)_{i=0}^{N-1}$ and $(t_i)_{i=0}^{N-1}$, then $\{a_i : j= 0,\ldots, N-1\}\subseteq \{t_i : i = 0, \ldots, N-1\}$ as sets, if and only if there exists a sequence $(s_i)_{i=0}^{2N-1}$ of double the size, which satisfies the lookup identity
\begin{equation}
\label{e:lookup:Gabizon}
\prod_{i=0}^{N-1} (X + s_{i}+ s_{i+1\bmod N}\cdot Y)
= \prod_{i=0}^{N-1}  (X + a_i + a_i\cdot Y)\cdot (X + t_i + t_{i+1\bmod N}\cdot Y).
\end{equation}
(The sequence $(s_i)_{i=0}^{2N-1}$ is  the concatenation of the $(a_i)$ and $(t_i)$, ordered by value in the same way as given by $t$.)
We again discuss two approaches\footnotemark for dealing with the grand product obtained from \eqref{e:lookup:Gabizon}, when sampling random $(\alpha, \beta)\sample F^2$ for $(X,Y)$.
\footnotetext{%
We point out that the presented strategies slightly differ from the one in \cite{Hyperplonk}, which uses the more expensive grand product argument from \cite{Quarks}.
}
The first one applies a batched grand product argument over $H$, independent of $M$ the number of columns, leading to a $\bigO{M^2}$ prover for similar reasons as Protocol \ref{prot:lookup} does.
The second one proves the grand product argument over an extended hypercube $\bar H$ of size $M\cdot |H|$, which on the one hand leads to ``larger'' functions to be provided, but on the other hand has a $\bigO M$ prover which outperforms the first approach at a high number of columns.

 
%In terms of performance both are equivalent.)

\subsection{The argument for not too many columns}
\label{s:hyperplonk:small}

Let $t: H'\rightarrow F$ be the lookup table, and $f_i:  H'\rightarrow F$, $i=1,\ldots,M$, the functions subject to the lookup.
Although the functions are defined over the punctuated hypercube $H'$, we assume arbitrary values at $\vec 1$.
(These will be ignored by the lookup argument.)
The prover provides the ordered merge of the $f_i$  together with $t$ via additional functions $s_i: H'\rightarrow F$, $i=1,\ldots, M+1$, subject to the lookup identity
\begin{multline*}
\prod_{\vec x\in H'} \prod_{i=1}^{M} (X + s_{i}(\vec x) + s_{i+1}(\vec x)\cdot Y)\cdot (X + s_{M+1}(\vec x) + s_1(T(\vec x))\cdot Y)
\\
= \prod_{\vec x\in H'} \prod_{i=1}^{M} (X + f_i(\vec x) + f_i(\vec x)\cdot Y)\cdot (X + t(\vec x) + t(T(\vec x))\cdot Y).
\end{multline*}
The identity is reduced to a grand product over $H'$ using random samples $\alpha, \beta\sample F$ for $X$ and $Y$, yielding 
\[
\prod_{\vec x\in H'} h(\vec x) = 1,
\]
where 
\[
h(\vec x) = \frac{\alpha + s_{M+1}(\vec x) + s_1(\sigma(\vec x))\cdot \beta}{\alpha + g(\vec x) + g(\sigma(\vec x))\cdot \beta}\cdot 
\prod_{i=1}^M \frac{(\alpha + s_i(\vec x) + s_{i+1}(\vec x)\cdot \beta)}{
(\alpha + f_i(\vec x) + f_i(\vec x)\cdot \beta)}.
\]
%\[
%\prod_{\vec x\in H'}\frac{\alpha + s_{M+1}(\vec x) + s_1(T(\vec x))\cdot \beta}{\alpha + t(\vec x) + t(T(\vec x))\cdot \beta}\cdot 
%\prod_{i=1}^M \frac{(\alpha + s_i(\vec x) + s_{i+1}(\vec x)\cdot \beta)}{
%(\alpha + f_i(\vec x) + f_i(\vec x)\cdot \beta)} = 1.
%\]

%The lookup IOP is as follows. 
%(The IOP in \cite{Hyperplonk} uses the grand product argument from \cite{Quarks}, but the protocol is easily improved by taking over the univariate strategy)
The prover computes the cumulative products of the values $h(\vec x)$ along the orbit of the time shift $T$ on $H'$, starting with $\phi(-\vec 1) = 1$, and setting
\[
\phi\big(T^{k}(-\vec 1)\big) = \phi\big(T^{k-1}(-\vec 1)\big) \cdot h\big(T^{k-1}(-\vec 1)\big),
\]
for $k=1, \ldots, |H|-1$.
At the remaining point $\vec x = \vec 1$ outside $H'$, we set $\phi(\vec x)$ to zero.
Correctness of the grand product is proven by the constraint on its initial value $\phi(-\vec 1) = 1$, and the domain identity
\begin{equation}
\phi(T( \vec x)) \cdot  \tau(\vec x)\cdot \prod_{i=1}^M  \varphi_{i}(\vec x) 
- \phi(\vec x)\cdot \sigma_{M+1}(\vec x)\cdot \prod_{i=1}^M \sigma_{i}(\vec x)
= 0, 
\end{equation}
for all $\vec x \in H$, where for $i=1,\ldots, M$, we use the notation
\begin{align*}
\varphi_{i} (\vec x) &=\alpha +  (1+\beta)\cdot f_i(\vec x),
\\
\sigma_{i}(\vec x) &= \alpha + s_i(\vec x) + \beta\cdot s_{i+1}(\vec x), 
\end{align*}
and
$\sigma_{M+1}(\vec x) =  \alpha + s_{M+1}(\vec x) + \beta\cdot s_1(T( \vec x))$,
$\tau(\vec x) = \alpha + t(\vec x) + \beta\cdot t(T( \vec x))$.
As in Protocol \ref{prot:lookup}, both constraints on $\phi$ are reduced to sumchecks over $H$ by help of the Lagrange polynomials $L_H(\:.\:, \vec y)$, $\vec y\sample F^n$, and $L_H(\:.\:, -\vec 1)$, and combined into a single one by a batching randomness $\lambda\sample F$.
The resulting overall sumcheck is
\[
\sum_{\vec x\in H} Q(L_H(\vec x, \vec y), L_H(\vec x, -\vec 1),  \tau(\vec x), \varphi_{1}(\vec x),\ldots, \varphi_{M}(\vec x), \sigma_{1}(\vec x),\ldots, \sigma_{M+1}(\vec x),  \phi(\vec x), \phi(T(\vec x))) = 0,
\]
where
\begin{equation}
\label{e:lookup:hyperplonk:Q}
Q(L_H, L,   \tau, \varphi_1,\ldots, \varphi_M, \sigma_1,\ldots, \sigma_{M+1}, \phi, \phi_T) 
= L_H \cdot \Big(\phi_T \cdot \tau \cdot\prod_{i=1}^M \varphi_i
- \phi \cdot \prod_{i=1}^{M+1} \sigma_i \Big)
+ \lambda\cdot L \cdot (\phi - 1). 
\end{equation}
Note that $Q$ has  absolute degree $d = M+3$, which is the same as in Protocol \ref{prot:lookup}, but  about the double of variables, $\nu= 2\cdot M+ 6$.
Its arithmetic complexities are $|Q_\mathsf M|= 2\cdot(M+1) + 3$, $|Q_\mathsf S|= 2$, $|Q_\mathsf A|= 1$. 



\subsubsection*{Prover costs}

The prover cost are as follows.
Computing the values for $\tau$ and all $\varphi_i,  \sigma_i$ over $H$ consumes overall
\[
|H|\cdot (2\cdot(M+1)\cdot \mathsf M + (3 M+ 4)\cdot \mathsf A),
\]
and $\phi(x)$ over $H$ is derived by another  $|H|\cdot (2\cdot M + 3)\cdot \mathsf M$ (again, using batch inversion). 
The domain evaluation for $L_H(\vec X, \vec y)$ is obtained within $|H|\cdot (\mathsf M + \mathsf A)$ operations, and the sumcheck costs
\begin{equation*}
|H| \cdot \left(1-\frac{1}{|H|}\right)\cdot \big((4\cdot M^2 + 25\cdot M + 38)\cdot \mathsf M +  (6\cdot M + 20)\cdot\textsf S + (2\cdot M^2 + 14\cdot M + 26) \cdot \textsf A\big).
\end{equation*}
Neglecting the $\nicefrac{1}{|H|}$-term, the overall cost of the prover is 
\begin{equation}
\label{e:hyperplonk:cost}
|H| \cdot  \big((4\cdot M^2 + 27\cdot M + 40)\cdot \mathsf M +  (6\cdot M + 20)\cdot\textsf S + (2\cdot M^2 + 17\cdot M + 30) \cdot \textsf A\big).
\end{equation}
This is comparable with \eqref{e:lookup:cost}, but does not take into account that the prover needs to supply a total of $M+1$ functions over $H$ instead of two. 


\subsection{A variant for a large number of columns}
\label{s:hyperplonk:large}

Suppose that we have a table function $t:H'\rightarrow F$ defined over the punctuated hypercube $H' = H\setminus\{\vec 1\}$, and $M$ column functions $f_1,\ldots, f_M: H\rightarrow F$ defined on the \textit{entire} hypercube.
%Again, $t$ takes an arbitrary value at $\vec 1$, which is ignored by the lookup 
We assume that  $M + 1=2^m $ for an integer $m\geq 1$, so that we can index the column functions as $f_{\vec z}$, with $\vec z$ from $\{\pm 1\}^m \setminus \{\vec 1\}$.
The ordered concatenation of the column functions and the table function has now $2^m \cdot |H| - 1$ entries, which we arrange in a single function $s$ over $\bar H\setminus \{\vec 1\}$ along the orbit of the time shift $T_{\bar H}$ on $\bar H$, where  $\bar H = \{\pm 1\}^m\times H$.
The value of $s$ at $\vec 1$ can be set arbitrary (it will be ignored by the lookup argument).
Using the patched  function
\[
f(\vec y, \vec x) = \sum_{\vec z \in \{\pm 1\}^m\setminus\{\vec 1\}} L_m(\vec y, \vec z)\cdot f_{\vec z}(\vec x),
\]
over $\bar H$, where $L_m$ is the Lagrange kernel for $\{\pm 1\}^m$, the Gabizon identity at random $(\alpha,\beta)\sample F^2$ reads now as
\[
\prod_{(\vec y, \vec x)\in \bar H'}  (\alpha + s(\vec y, \vec x) + \beta\cdot s(T_{\bar H} (\vec y, \vec x))) = \prod_{(\vec y, \vec x)\in \bar H'}  (\alpha + (1 + \beta)\cdot f(\vec y, \vec x) + L_{m}(\vec y, \vec 1)\cdot (t(\vec x) + \beta \cdot t(T_H(\vec x))),
\]
where $\bar H' = \bar H\setminus\{\vec 1\}$.
(Notice that the $\alpha$-term for $t$ on the right hand side is combined with the $\alpha$-term for $f$.)
In other words the prover needs to show that
\[
\prod_{(\vec y, \vec x)\in \bar H'}\frac{\varphi(\vec y, \vec x)}{\sigma(\vec y,\vec x)} = 1,
\]
where 
\begin{align*}
\varphi(\vec y, \vec x) &= \alpha + (1+\beta)\cdot f(\vec y, \vec x) + L_m(\vec y, \vec 1)\cdot (t(\vec x) + \beta \cdot t(T_H(\vec x))), 
\\
\sigma(\vec y, \vec x) &= \alpha + s(\vec y, \vec x) + \beta\cdot s(T_{\bar H} (\vec y, \vec x)).
\end{align*} 

As before, the prover provides a function $\phi$ over $\bar H'$ for the cumulative products of the values of $h(\vec x) = \frac{\varphi(\vec y, \vec x)}{\sigma(\vec y,\vec x)}$ along the orbit of $T_{\bar H}$, starting with $\phi(-\vec 1)= 1$, and setting
\[
\phi(T_{\bar H}^k (-\vec 1)) = \phi(T_{\bar H}^{k-1} (-\vec 1)) \cdot h(T_{\bar H}^{k-1} (-\vec 1)), 
\] 
for $k=1, \ldots, |\bar H| - 2$. 
At the remaining point $\vec 1$ we set $\phi(\vec 1) = 0$. %, so that the above recursive conditions holds even over the orbit of $\vec 1$, regardless of the values  $t(\vec 1)$ and $s(\vec 1, \vec 1)$.
Correctness of $\phi$ is ensured by $\phi(-\vec 1) = 1$, and the domain identity
\begin{equation*}
\phi(T_{\bar H}(\vec y, \vec x)) \cdot \varphi(\vec y, \vec x)
- \phi(\vec y, \vec x) \cdot \sigma(\vec y, \vec x) = 0
\end{equation*}
over $\bar H$.
Applying the Lagrange polynomials $L_{\bar H}(\:.\:, \vec z)$, $\vec z \sample F^{m+n}$, and $L_{\bar H}(\:.\:, -\vec 1)$, the constraints on $\phi$ are reduced to sumchecks over $\bar H$, which are then combined into a single one using a random $\lambda\sample F$.
The overall sumcheck is
\[
\sum_{(\vec y, \vec x)\in \bar H} Q(L_{\bar H}((\vec y, \vec x) , \vec z), L_{\bar H}((\vec y, \vec x), -\vec 1), \varphi(\vec y, \vec x), \sigma(\vec y, \vec x), \phi(\vec y, \vec x),  \phi(T_{\bar H}(\vec y, \vec x))) = 0,
\]
where 
\begin{equation}
Q(L_H, L, \varphi, \sigma,\phi, \phi_T) = 
L_H\cdot (\phi_T\cdot \varphi - \phi\cdot \sigma) + \lambda\cdot L\cdot (\phi - 1).
\end{equation}
The polynomial $Q$ has $\nu=6$ variables, absolute degree $d=3$, and its arithmetic complexities are $|Q_\mathsf M|= 5$, $|Q_\mathsf S| = 2$, $|Q_\mathsf A|= 1$.




\subsubsection*{Prover cost}

Computing the values of the patched function $\varphi$  over $\bar H$ takes 
\[
|\bar H |\cdot (1\cdot \mathsf M + 2\cdot \mathsf A),
\]
and the same number of operations are needed for $\sigma$.
The quotient $h$ over $\bar H$ is obtained by batch inversion, consuming
\[
|\bar H|\cdot (3 + 1) \cdot \mathsf M,
\]
and the values of $\phi$ are computed within another $|\bar H|$ multiplications.
Last but not least, computing $L_{\bar H}(\:.\:, \vec z)$ over $\bar H$ costs $|\bar H|\cdot (\mathsf M + \mathsf A)$, and the sumcheck 
\[
|\bar H| \cdot \left(1 + \frac{1}{|\bar H|}\right) \cdot (38 \cdot \mathsf M + 20 \cdot \mathsf S + 26\cdot\mathsf A).
\] 
Neglecting the $\frac{1}{|\bar H|}$-term, the overall cost of the prover is
\begin{equation}
\label{e:hyperplonk:large:cost}
(M+1)\cdot |H| \cdot (45 \cdot \mathsf M + 20 \cdot \mathsf S + 30\cdot\mathsf A),
\end{equation}
whereas it needs to provide the functions $s$ and $\phi$ over $\bar H$, which amounts the equivalent of $2\cdot (M+1)$ functions over $|H|$.

Based on our benchmark-backed equivalent number of field operations for a multi-scalar multiplications (Table \ref{tab:pippenger}), we obtain the following break even points.
\begin{table}[h!]
\caption{%
The estimated number of columns $M$ where the $\bigO{M}$ strategy starts to perform better than the protocol from Section \ref{s:hyperplonk:small}. 
The numbers are based on the operation counts \eqref{e:hyperplonk:cost} and \eqref{e:hyperplonk:large:cost} for the oracle prover, and the benchmarks for a multi-scalar multiplication over the Pallas curve, see Section \ref{s:appendix:benches}.
}
\vspace*{0.5cm}
\centering
\begin{tabular} {|c|c|c|c|c|}
\hline
$\log|H|$ & 12 & 14 & 16 & 18
\\\hline
$M$ & 143 & 120 & 103 & 92
\\\hline
\end{tabular}
\end{table}

\subsection{Comparison with logarithmic derivative lookups}

The main advantage of logarithmic derivative lookups over the ones from the current section is not the arithmetic cost  as reflected by the operation counts  \eqref{e:lookup:cost}, \eqref{e:lookup:large:cost} and \eqref{e:lookup:cost}, \eqref{e:lookup:large:cost}, but the number and sizes of oracle functions the prover needs to provide.
That oracle costs depend on the used commitment scheme.
To give an estimate for these costs in the case of an elliptic curve based Lagrange commitment scheme (such as the IPA), we 
rely on a benchmark-backed equivalent of field multiplications for the multi-scalar multiplication in the Pallas curve.
These are as follows.

\begin{table}[h!]
\caption{%
Benchmark of Halo2's Pippenger multi-scalar multiplication in the Pallas curve, varying the number $N$ of scalars.
The benchmarks where done on a AMD Ryzen 7 PRO 4750U, 32GB RAM DDR4,  restricting to a single core.
}
\label{tab:pippenger}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
$\log N$ &  Pippenger of size $N$ & $2^8\cdot N$ field mult. & equivalent field mult.
\\\hline
12 & $46.010$ ms &  $21.11$ ms & $N\cdot 557\cdot \mathsf M$
\\
14 & $153.67$ ms & $42.315$ ms & $N\cdot 464 \cdot \mathsf M$
\\
16 & $522.13$ ms & $169.70$ ms & $N\cdot 394\cdot\mathsf M$
\\
18 &       $1.869$  ms  & $679.27$ ms & $N\cdot 351\cdot\mathsf M$
\\\hline
\end{tabular}
\end{center}
\end{table}

With these equivalence measure and the operation counts from  \eqref{e:lookup:cost}, \eqref{e:lookup:large:cost} and \eqref{e:hyperplonk:cost}, \eqref{e:hyperplonk:large:cost}, we obtain the following ratios of field multiplications.

\begin{table}[h!]
\caption{%
The estimated performance advantage of the logarithmic derivative lookups over the ones from this section, as the ratio of their number of field multiplications $r =\nicefrac{\mathsf M({Plookup})}{\mathsf M({logD})}$.
The numbers are based on the equivalent number of field multiplication from Table \ref{tab:pippenger}.
For hypercube sizes $|H|$ ranging from $2^{12}$--$2^{18}$ we describe the maximum ratio $r_{max}$ over the number of columns $M$, as well as the ranges for $M$ over which $r$ is larger than $2$ and $3$, respectively.
The minimum ratio is throughout $r= 1.5$ and obtained in the single-column setting $M=1$. 
}
\label{tab:comparison}
\vspace*{0.5cm}
\centering
\begin{tabular} {|c|c|c|c|c|}
\hline
$\log|H|$ & $r\geq 3$ & $r\geq 2$ & $r_{max}$ 
\\\hline
12 & $M\in [5, 41]$ & $M\in [3, 87]$ & $4.1$ (at $M =15$)
\\
14 & $M\in [6, 32]$ & $M\in [3, 71]$ & $3.8$ (at $M= 13$)
\\
16 & $M\in [6, 26]$  & $M\in [3, 59]$ & $3.5$ (at $M= 12$)
\\
18 & $M\in [6, 21]$  & $M\in [3, 52]$ & $3.2$ (at $M= 12$)
\\\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{alpha}
\bibliography{bibfileSNARKs}

\appendix
%\newpage
\section{Appendix}
\label{s:appendix}


\subsection{Univariate Lookups}
\label{s:uv:lookup}

Let us sketch the optimized variant of the lookup IOP from \cite{Plookup} described in the blog post \cite{LookupsBlog}.
As in the previous section, $H$ denotes the univariate domain $H = \{x\in F: x^n = 1\}$ with generator $g$.
Suppose that $f:H\rightarrow F$ is  a function for which we want to prove that its range is contained in that of $t: H\rightarrow F$, i.e. $\{f(x)\}_{x\in H}\subseteq \{t(x)\}_{x\in H}$ as sets (with multiple recurrences removed).
Let $(f_i)_{i=0}^{n-1}$ and $(t_i)_{i=0}^{n-1}$ denote the sequences  of values given by the index with respect to $g$, i.e. $f_i= f(g^i)$ and $t_i=f(g^i)$, and let $m_i$ be the multiplicity of $t_i$ in the sequence $(f_i)$.
Consider the sequence
\[
\bar s = (\bar s_i)_{i=0}^{2n - 1} = (\underbrace{t_0, \ldots, t_0}_{1 + m_0 \text{ times}}, \underbrace{t_1, \ldots, t_1}_{1 + m_1 \text{ times}}, \ldots, \underbrace{t_{n-1}, \ldots, t_{n-1}}_{1 + m_{n-1} \text{ times}}),
\]
and split it into $s_0 = (\bar s_{2\cdot i})_{i=0}^{n-1}$ and  $s_1 = (\bar s_{2\cdot i+ 1})_{i=0}^{n-1}$, which we again regard as functions on $H$.
Then
\begin{equation}
\label{e:uv:lookup:multiset}
\{(s_0 (x), s_1(x)), (s_1(x), s_0(g\cdot x))\}_{x\in H} = \{ (f(x), f(x))\}_{x\in H} \cup \{(t(x), t(g\cdot x))\}_{x\in H}
\end{equation}
as multisets.
Moreover,  $\{f(x)\}_{x\in H}\subseteq \{t(x)\}_{x\in H}$ if and only if there exists functions $s_0, s_0: H\rightarrow F$ satisfying the multiset identity \eqref{e:uv:lookup:multiset}.
See  \cite{LookupsBlog} for details.
The multiset equation \eqref{e:uv:lookup:multiset} is equivalent to the formal identity
\begin{multline*}
\prod_{x\in H} (X + s_0(x) + s_1(x)\cdot Y)\cdot (X + s_1(x) + s_0(g\cdot x)\cdot Y) 
\\
= \prod_{x\in H} (X + f(x) + f(x)\cdot Y)\cdot (X + t(x) + t(g\cdot x)\cdot Y),
\end{multline*}
which is reduced by random sampling $\alpha, \beta\sample F$ for $X$ and $Y$, to the grand product
\begin{equation}
\label{e:UV:lookup:q}
\prod_{x\in H}  \frac{s_{0,\alpha,\beta}(x)\cdot s_{1,\alpha,\beta}(x)}{f_{\alpha,\beta}(x) \cdot t_{\alpha,\beta}(x)} = 1,
\end{equation}
where
\begin{align*}
f_{\alpha,\beta} (x) &=\alpha +  (1+\beta)\cdot f(x),
\\
t_{\alpha, \beta}(x) &= \alpha + t(x) + \beta\cdot t(g\cdot x),
\\
s_{0,\alpha,\beta}(x) &= \alpha + s_0(x) + \beta\cdot s_1(x),
\\
s_{1,\alpha,\beta}(x) &=  \alpha + s_1(x) + \beta\cdot s_0(g\cdot x).
\end{align*}
To prove the grand product, the prover sets $\phi(1) = 1$, and computes recursively $\phi(g^i )= \phi(g^{i-1})\cdot q(g^{i-1})$, for $i=1,\ldots, n-1$.
(The grand product is equal to $1$ if and only if $\phi(g^n) = \phi(1) = 1$.)
This leads to the following domain identities over $H$,
\begin{multline}
\phi(g\cdot x) \cdot (\alpha + f(x)\cdot (1+\beta))\cdot (\alpha + t(x) + t(g\cdot x)\cdot \beta) 
\\
- \phi(x)\cdot (\alpha + s_0(x) + s_1(x)\cdot \beta)\cdot (\alpha + s_1(x) + s_0(g\cdot x)\cdot \beta)
= 0,
\end{multline}
and
\begin{equation}
L_H(x, 1)\cdot (\phi(x) - 1) = 0,
\end{equation}
where $L_H(X, Y) = \frac{1}{n}\cdot \frac{Y\cdot v_H(X) - X\cdot v_H(Y)}{X - Y}$ is the Lagrange kernel for the univariate domain $H$.
The indentities are combined into a single one using a random $\lambda \sample F$, yielding the overall polynomial identity is
\begin{multline*}
\phi(g\cdot X) \cdot (\alpha + f(X) + f(X)\cdot \beta)\cdot (\alpha + t(X) + t(g\cdot X)\cdot \beta) 
\\
- \phi(X) \cdot (\alpha + s_0(X) + s_1(X)\cdot \beta)\cdot (\alpha + s_1(X) + s_0(g\cdot X)\cdot \beta) 
\\
+  \lambda \cdot L_H(X, 1)\cdot (\phi(X) - 1) = 0  \mod v_H(X),
\end{multline*}
where $v_H(X)$ is the vanishing polynomial of $H$.

%Let us discuss the prover cost of this polynomial IOP.
%Computing $f_{\alpha,\beta}, t_{\alpha,\beta}, s_{0,\alpha,\beta}, s_{1,\alpha,\beta}$ over $H$ consumes 
%\[
%|H|\cdot (4\cdot \mathsf M + 7 \mathsf A),
%\]
%and $\phi(x)$ over $H$ is derived by another 
%\[
%6\cdot |H|\cdot \mathsf M,
%\] 
%using batch inversion. 
%The domain evaluation for $L_H(x, y) = \frac{1}{n}\cdot \frac{- x \cdot v_H(y)}{x- y}$ is obtained within $3\cdot |H|\cdot (3\cdot \mathsf M + \mathsf S)$ operations (again, using batch inversion).
%The coefficients of $f(X)$, $\phi(X), s_0(X), s_1(X)$ are obtained in overall  
%\[
%4\cdot |H|\cdot \log|H|\cdot (\mathsf M + \mathsf A).
%\] 
%The largest part of the prover costs is the computation of the overall quotient $h(X)$.
%Its values are computed over a suitably sized multiplication domain. 
%For simplicity, we assume its size as $3\cdot |H|$.
%The steps for computing the values of $h$ are as follows.
%\begin{itemize}
%\item 
%The values of $f(X)$, $\phi(X), s_0(X), s_1(X)$ over the two non-trivial cosets of $H$ in the multiplication domain are computed by $4$ coset FFT's, which cost overall
%\[
%4\cdot 2\cdot ( |H| \cdot \textsf M + |H|\log|H|\cdot (\mathsf M + \mathsf A))
%\] 
%\item
%The values of the numerator and denominator of $q(X)$ over the two remaining cosets of $H$ cost
%\[
%2\cdot (6 \cdot |H|\cdot \mathsf M +  |H| \cdot 7\cdot \mathsf A)  
%\]
%and the left hand side of the overall identity over the multiplication domain costs 
%\[
%3\cdot |H|\cdot (4 \cdot\mathsf M + 1\cdot\mathsf S + 1\cdot \mathsf A).
%\]
%using already existing domain evaluations.
%\end{itemize}
%The coefficients of the left hand side of the overall identity are computed within
%\[
%3\cdot |H| \cdot \log(3\cdot |H|)\cdot (\mathsf M + \mathsf A),
%\]
%and polynomial division by $v_H(X)= X^n - 1$ costs only $|H|\cdot \mathsf A$.
%Hence the total cost of the IOP is 
%\begin{equation}
%\label{e:UV:lookup:cost}
%|H|\cdot  (15\cdot \log |H| + 3\log 3 + 51) \cdot (\mathsf M + \mathsf A) + 3\cdot |H|\cdot\mathsf S + 24\cdot |H|\cdot \mathsf A,
%\end{equation}
%including the cost of computing the coefficients for $h(X)$, and 
%\begin{equation}
%\label{e:UV:lookup:cost:without:h}
%|H|\cdot  (12\cdot \log |H|  + 51) \cdot (\mathsf M + \mathsf A) + 3\cdot |H|\cdot\mathsf S + 24\cdot |H|\cdot \mathsf A,
%\end{equation}
%without.
%In the course of the protocol the prover provides four polynomials of size $|H|$, which are $s_0(X), s_1(X), \phi(X)$ and $h(X)$.


%\subsection{RapidUp}
%
%RapidUp \cite{Rapidup} is not sound, and there does not seem a way to patch their protocol.
%The select the linear factors in the grand product,
%\[
%\prod_{x\in H} (s_f(x) \cdot (X + f(x)) + (1-s_f(x))\cdot 1)
%= \prod_{x\in H} \left(s_{t}(x)\cdot (X + t(x)) + (1 - s_t(x))\cdot 1\right),
%\]
% where at least one has to impose the Boolean constraint on $s_f(x)$ and $s_t(x)$, i.e. $s_f(x)\cdot (1 - s_f(x)) = 0$, $s_t(x)\cdot (1 - s_t(x)) = 0$.
% But even then, one might set the selector for $f$ in such a way that it mutes values $f(x)$ which are not contained in the range of $g$.






\end{document}
